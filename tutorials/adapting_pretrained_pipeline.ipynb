{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyannote/pyannote-audio/blob/develop/tutorials/adapting_pretrained_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzkG4poB2BH"
      },
      "source": [
        "# Adapting pyannote.audio 2.1 pretrained speaker diarization pipeline to your own data\n",
        "\n",
        "> \"How I reached 1st place at Ego4D 2022, 1st place at Albayzin 2022, and 6th place at VoxSRC 2022 speaker diarization challenges\"\n",
        "\n",
        "[pyannote.audio](https://github.com/pyannote/pyannote-audio) is an open-source toolkit written in Python for speaker diarization. \n",
        "\n",
        "Version 2.1 introduces a major overhaul of the default speaker diarization pipeline, made of three main stages: speaker segmentation applied to a short sliding window, neural speaker embedding of each (local) speakers, and (global) agglomerative clustering.\n",
        "\n",
        "Despite its decent out-of-the-box performance, the default pipeline may suffer from the domain mismatch problem (common to most machine learning models) and not perform well on your own data. This tutorial will guide you through two recipes to adapt it to your own data and (hopefully) get better performance. Depending on the number and duration of labeled conversations, you may either focus on optimizing hyper-parameters or additionally fine-tune the internal speaker segmentation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnaQ4JSLF8Ms"
      },
      "source": [
        "⚠ Make sure that you switch to a GPU runtime (Runtime > Change runtime type).  \n",
        "If you don't, everything will be extremely slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZjbjOBBDrdm"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Let's start by installing `pyannote.audio` 2.1.1 (and `rich` for pretty progress bars)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kYxY82Jzz3s"
      },
      "outputs": [],
      "source": [
        "!pip install -qq pyannote.audio==2.1.1\n",
        "!pip install -qq rich"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQ10VIf2W1c"
      },
      "source": [
        "⚠ Restart the runtime (Runtime > Restart runtime).  \n",
        "If you don't, `pyannote.database` will throw an error below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz-b8j6RD7H6"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "First things first: we need data... Annotated data! Ideally, lots of annotated data! \n",
        "\n",
        "For the purpose of this tutorial, we will rely on the AMI-SDM (single distance microphone) corpus. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSQVKFDC0cOe"
      },
      "outputs": [],
      "source": [
        "# download AMI-SDM mini corpus\n",
        "%cd /content/\n",
        "!git clone https://github.com/pyannote/AMI-diarization-setup\n",
        "%cd /content/AMI-diarization-setup/pyannote/\n",
        "!bash download_ami_sdm_mini.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTKkOeMr2QUL",
        "outputId": "d9cc39fd-4cc2-44fe-a922-533370221d31"
      },
      "outputs": [],
      "source": [
        "!PYANNOTE_DATABASE_CONFIG=\"/content/AMI-diarization-setup/pyannote/database.yml\" pyannote-database info AMI-SDM.SpeakerDiarization.mini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09LrQFIfp0zC"
      },
      "source": [
        "Note that we use a \"mini\" version of AMI-SDM so that the tutorial can be run in half an hour but the full version is also available for you to get better results. \n",
        "\n",
        "If you want to try it, replace `download_ami_sdm_mini.sh` by `download_ami_sdm.sh` and `AMI-SDM.SpeakerDiarization.mini` by `AMI-SDM.SpeakerDiarization.only_words` and you are good to go! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6V8Exw41XBp"
      },
      "outputs": [],
      "source": [
        "from pyannote.database import registry, FileFinder\n",
        "registry.load_database(\"./AMI-diarization-setup/pyannote/database.yml\")\n",
        "dataset = get_protocol(\"AMI-SDM.SpeakerDiarization.mini\", {\"audio\": FileFinder()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HivpZEgZEVAu"
      },
      "source": [
        "## Pretrained pipeline\n",
        "\n",
        "Let's start by running the pretrained pipeline on the test set and evaluate its performance.\n",
        "\n",
        "Official [pyannote.audio](https://github.com/pyannote/pyannote-audio) pipelines (i.e. those under the [`pyannote` organization](https://hf.co/pyannote) umbrella) are open-source, but gated. It means that you have to first accept users conditions on their respective Huggingface page to access the pretrained weights and hyper-parameters. \n",
        "\n",
        "To load the speaker diarization pipeline used in this tutorial, you have to \n",
        "* visit [hf.co/pyannote/speaker-diarization](https://hf.co/pyannote/speaker-diarization) and accept the terms\n",
        "* visit [hf.co/pyannote/segmentation](https://hf.co/pyannote/segmentation) (used internally by the speaker diarization pipeline)and accept the terms\n",
        "* log in using `notebook_login`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbXEQUGXscTQ"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7eN_Y792Cxt"
      },
      "outputs": [],
      "source": [
        "from pyannote.audio import Pipeline\n",
        "pretrained_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDIIp7iaICUC",
        "outputId": "b4880a85-692b-470d-f133-d95128bdecd9"
      },
      "outputs": [],
      "source": [
        "# this takes approximately 2min to run on Google Colab GPU\n",
        "from pyannote.metrics.diarization import DiarizationErrorRate\n",
        "metric = DiarizationErrorRate()\n",
        "\n",
        "for file in dataset.test():\n",
        "    # apply pretrained pipeline\n",
        "    file[\"pretrained pipeline\"] = pretrained_pipeline(file)\n",
        "\n",
        "    # evaluate its performance\n",
        "    metric(file[\"annotation\"], file[\"pretrained pipeline\"], uem=file[\"annotated\"])\n",
        "\n",
        "print(f\"The pretrained pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric):.1f}% on {dataset.name} test set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "r4UydYQkKsxW",
        "outputId": "7069d8e5-7dc7-4701-bcd5-1663998cfd1d"
      },
      "outputs": [],
      "source": [
        "file[\"annotation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "s4C7xMQlKwVX",
        "outputId": "930c571f-069b-44ad-f87e-ce8d471176a6"
      },
      "outputs": [],
      "source": [
        "file[\"pretrained pipeline\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrYuCLLUKALA"
      },
      "source": [
        "## Fine-tuning the segmentation model\n",
        "\n",
        "When a sufficiently large training set of labeled conversations is available, fine-tuning the internal speaker segmentation model may lead to significant performance boost. \n",
        "\n",
        "Starting from the pretrained model..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeUDgr4f55v6"
      },
      "outputs": [],
      "source": [
        "from pyannote.audio import Model\n",
        "model = Model.from_pretrained(\"pyannote/segmentation\", use_auth_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAyCf7ontua_"
      },
      "source": [
        "... we prepare it for fine-tuning on the training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk_a7ABQ6PPH"
      },
      "outputs": [],
      "source": [
        "from pyannote.audio.tasks import Segmentation\n",
        "task = Segmentation(\n",
        "    dataset, \n",
        "    duration=model.specifications.duration, \n",
        "    max_num_speakers=len(model.specifications.classes), \n",
        "    batch_size=32,\n",
        "    num_workers=2, \n",
        "    loss=\"bce\", \n",
        "    vad_loss=\"bce\")\n",
        "model.task = task\n",
        "model.prepare_data()\n",
        "model.setup(stage=\"fit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgobhVTKt9sH"
      },
      "source": [
        "The actual training is done with [`lightning`](https://github.com/Lightning-AI/lightning):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_bVYrNo6TmI"
      },
      "outputs": [],
      "source": [
        "# this takes approximately 15min to run on Google Colab GPU\n",
        "from types import MethodType\n",
        "from torch.optim import Adam\n",
        "from pytorch_lightning.callbacks import (\n",
        "    EarlyStopping,\n",
        "    ModelCheckpoint,\n",
        "    RichProgressBar,\n",
        ")\n",
        "\n",
        "# we use Adam optimizer with 1e-4 learning rate\n",
        "def configure_optimizers(self):\n",
        "    return Adam(self.parameters(), lr=1e-4)\n",
        "\n",
        "model.configure_optimizers = MethodType(configure_optimizers, model)\n",
        "\n",
        "# we monitor diarization error rate on the validation set\n",
        "# and use to keep the best checkpoint and stop early\n",
        "monitor, direction = task.val_monitor\n",
        "checkpoint = ModelCheckpoint(\n",
        "    monitor=monitor,\n",
        "    mode=direction,\n",
        "    save_top_k=1,\n",
        "    every_n_epochs=1,\n",
        "    save_last=False,\n",
        "    save_weights_only=False,\n",
        "    filename=\"{epoch}\",\n",
        "    verbose=False,\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=monitor,\n",
        "    mode=direction,\n",
        "    min_delta=0.0,\n",
        "    patience=10,\n",
        "    strict=True,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "callbacks = [RichProgressBar(), checkpoint, early_stopping]\n",
        "\n",
        "# we train for at most 20 epochs (might be shorter in case of early stopping)\n",
        "from pytorch_lightning import Trainer\n",
        "trainer = Trainer(accelerator=\"gpu\", \n",
        "                  callbacks=callbacks, \n",
        "                  max_epochs=20,\n",
        "                  gradient_clip_val=0.5)\n",
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "830LvZfdd3Rn"
      },
      "outputs": [],
      "source": [
        "# save path to the best checkpoint for later use\n",
        "finetuned_model = checkpoint.best_model_path\n",
        "\n",
        "# uncomment to download the checkpoint\n",
        "#from google.colab import files\n",
        "#files.download(finetuned_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NVGAIMd-uPI"
      },
      "source": [
        "## Optimizing the pipeline hyper-parameters\n",
        "\n",
        "The pretrained `pyannote/speaker-diarization` pipeline relies on its own set of hyper-parameters adapted to the internal `pyannote/segmentation` pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OUIBHP7-xk_",
        "outputId": "20b3af76-d2d3-4bf9-eb58-2589f8e86939"
      },
      "outputs": [],
      "source": [
        "pretrained_hyperparameters = pretrained_pipeline.parameters(instantiated=True)\n",
        "pretrained_hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1rRE4NqJmb5"
      },
      "source": [
        "There is no reason the above hyper-parameters are optimal for the newly finetuned speaker segmentation model. Let's optimize them:\n",
        "\n",
        "* `segmentation.threshold` ($\\theta$  in the [technical report](https://huggingface.co/pyannote/speaker-diarization/resolve/main/technical_report_2.1.pdf), between 0 and 1) controls the aggressiveness of speaker activity detection (i.e. a higher value will result in less detected speech); \n",
        "* `clustering.threshold` ($\\delta$  in the report, between 0 and 2) controls the number of speakers (i.e. a higher value will result in less speakers).\n",
        "* `segmentation.min_duration_off` ($\\Delta$ in the report, in seconds) controls whether intra-speaker pauses are filled. This usually depends on the downstream application so it is better to first force it to zero (i.e. never fill intra-speaker pauses) during optimization.\n",
        "* `clustering.centroid` is the linkage used by the agglomerative clustering step. `centroid` has been found to be slightly better than `average`. \n",
        "* `clustering.min_cluster_size` controls what to do with small speaker clusters. Clusters smaller than that are assigned to the most similar large cluster. `15` is a good default value.\n",
        "\n",
        "We start by optimizing `segmentation.threshold` by assuming that the subsequent clustering step is perfect (cf. `OracleClustering`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSWW7bIphgAI"
      },
      "outputs": [],
      "source": [
        "# this takes approximately 5min to run on Google Colab GPU\n",
        "from pyannote.audio.pipelines import SpeakerDiarization\n",
        "from pyannote.pipeline import Optimizer\n",
        "\n",
        "pipeline = SpeakerDiarization(\n",
        "    segmentation=finetuned_model,\n",
        "    clustering=\"OracleClustering\",  \n",
        ")\n",
        "# as reported in the technical report, min_duration_off can safely be set to 0.0\n",
        "pipeline.freeze({\"segmentation\": {\"min_duration_off\": 0.0}})\n",
        "\n",
        "optimizer = Optimizer(pipeline)\n",
        "dev_set = list(dataset.development())\n",
        "\n",
        "iterations = optimizer.tune_iter(dev_set, show_progress=False)\n",
        "best_loss = 1.0\n",
        "for i, iteration in enumerate(iterations):\n",
        "    print(f\"Best segmentation threshold so far: {iteration['params']['segmentation']['threshold']}\")\n",
        "    if i > 20: break   # 50 iterations should give slightly better results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAUx-1Pw3Uc9"
      },
      "source": [
        "Then, we use the optimized value of `segmentation.threshold` and optimize `clustering.threshold`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSzOWLL5Q29-"
      },
      "outputs": [],
      "source": [
        "best_segmentation_threshold = optimizer.best_params[\"segmentation\"][\"threshold\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_pQkyQ5RCjl"
      },
      "outputs": [],
      "source": [
        "# this takes approximately 5min to run on Google Colab GPU\n",
        "pipeline = SpeakerDiarization(\n",
        "    segmentation=finetuned_model,\n",
        "    embedding=pretrained_pipeline.embedding,\n",
        "    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n",
        "    clustering=pretrained_pipeline.klustering,\n",
        ")\n",
        "\n",
        "pipeline.freeze({\n",
        "    \"segmentation\": {\n",
        "        \"threshold\": best_segmentation_threshold,\n",
        "        \"min_duration_off\": 0.0,\n",
        "    },\n",
        "    \"clustering\": {\n",
        "        \"method\": \"centroid\",\n",
        "        \"min_cluster_size\": 15,\n",
        "    },\n",
        "})\n",
        "\n",
        "optimizer = Optimizer(pipeline)\n",
        "iterations = optimizer.tune_iter(dev_set, show_progress=False)\n",
        "best_loss = 1.0\n",
        "for i, iteration in enumerate(iterations):\n",
        "    print(f\"Best clustering threshold so far: {iteration['params']['clustering']['threshold']}\")\n",
        "    if i > 20: break  # 50 iterations should give slightly better results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDA2XT-wAzDO"
      },
      "source": [
        "Finally, we use the optimized values of `segmentation.threshold` and `clustering.threshold` to evaluate the performance of the finetuned pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBJv2j1U_5mj"
      },
      "outputs": [],
      "source": [
        "best_clustering_threshold = optimizer.best_params['clustering']['threshold']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir1VUqNEimcN",
        "outputId": "d8edfb16-8f97-4d84-d4d2-3066d8b7c791"
      },
      "outputs": [],
      "source": [
        "# this takes approximately 2min to run on Google Colab GPU\n",
        "finetuned_pipeline = SpeakerDiarization(\n",
        "    segmentation=finetuned_model,\n",
        "    embedding=pretrained_pipeline.embedding,\n",
        "    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n",
        "    clustering=pretrained_pipeline.klustering,\n",
        ")\n",
        "\n",
        "finetuned_pipeline.instantiate({\n",
        "    \"segmentation\": {\n",
        "        \"threshold\": best_segmentation_threshold,\n",
        "        \"min_duration_off\": 0.0,\n",
        "    },\n",
        "    \"clustering\": {\n",
        "        \"method\": \"centroid\",\n",
        "        \"min_cluster_size\": 15,\n",
        "        \"threshold\": best_clustering_threshold,\n",
        "    },\n",
        "})\n",
        "\n",
        "metric = DiarizationErrorRate()\n",
        "\n",
        "for file in dataset.test():\n",
        "    # apply finetuned pipeline\n",
        "    file[\"finetuned pipeline\"] = finetuned_pipeline(file)\n",
        "\n",
        "    # evaluate its performance\n",
        "    metric(file[\"annotation\"], file[\"finetuned pipeline\"], uem=file[\"annotated\"])\n",
        "\n",
        "print(f\"The finetuned pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric):.1f}% on {dataset.name} test set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "LFvNtdZTBDZh",
        "outputId": "f72da25d-7df7-4d8f-97b1-b5ca0f2febe7"
      },
      "outputs": [],
      "source": [
        "file[\"finetuned pipeline\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "odzLUrqaBHgV",
        "outputId": "1864ccc7-076e-41bf-ba63-13df32d57fda"
      },
      "outputs": [],
      "source": [
        "file[\"annotation\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muYEOZ36VJo5"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In just about half an hour (and 6 hours of training data), we managed to reduce the diarization error rate from 32.5% to 26.6%.\n",
        "\n",
        "[Yours truly](https://herve.niderb.fr) used this very recipe for their submissions to several speaker diarization benchmarks organized in 2022. I reached:\n",
        "\n",
        "* 6th place at [VoxSRC 2022](https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc) speaker diarization challenge\n",
        "* 1st place at [Ego4D 2022](https://ego4d-data.org/) audio-only speaker diarization challenge\n",
        "* 1st place at [Albayzin 2022](http://catedrartve.unizar.es/albayzin2022results.html) speaker diarization challenge\n",
        "\n",
        "The [technical report](https://huggingface.co/pyannote/speaker-diarization/resolve/main/technical_report_2.1.pdf) contains a detailed description of the pipeline, as well as an extensive evaluation of its performance on multiple benchmarking datasets.\n",
        "\n",
        "For technical questions and bug reports, please check [pyannote.audio](https://github.com/pyannote/pyannote-audio) Github repository so that my (or anyone's) public answer benefits other people as well. \n",
        "\n",
        "For scientific consulting enquiries, please contact [me](herve@niderb.fr)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMc87txSanOjHXPQP0Zpu9J",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
